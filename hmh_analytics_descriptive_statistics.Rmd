# Brian Buckley February 2015

# Descriptive Statistics

```{r}
# Things to consider:
# How many students did not pass anything
# What is the hardest content to pass
# What is the easiest content to pass
# Remove NA data
# What proportion of content was accessed
# What did Knewton recommend
```

```{r setOptions, message=FALSE}
library(googleVis)
op<-options(gvis.plot.tag='chart')
```

```{r tidy=TRUE}
str(dataAll)
```
There are **`r nrow(dataAll)`** Observations (rows)
Note that character data is a character vector  - we will need to convert these to R factors.  Also the 
binary data (*isComplete*, *isCorrect*) are strings rather than a logical value so again these have to be 
converted.

***

Now we can get summary statistics for the data

```{r tidy=TRUE}
summary(dataAll)
```
The character vector variables cannot be statistically summarized.  When we convert these to factors later we will be able to perform statistics on them.

***

## Students in each learningInstance i.e. class
```{r tidy=TRUE}
library(plyr)
classes<-count(dataAll, vars=c("studentId", "learningInstance"))
classTotals<-aggregate(freq ~ learningInstance, data=classes, sum)
classTotals<-classTotals[with(classTotals, order(-freq)),]
summary(classTotals$freq)
```
Note that we have classes with **thousands** of students and with just **one** student.  Are these QA data??  Classes with 32 or less students make up just the first quartile of the data.

***

## Histogram of class size distribution
```{r tidy=TRUE}
hist(classTotals$freq, col="lightblue", border="pink", xlab="# Students per class", ylab="# classes", main="Class Size Density")
```

***

Very skewed-to-the-right data!  Looks like we have QA data here?

Let's take the first quartile class size (32 students) and accept data with 10 or more.  Then plot it
on a Google Visualizations histogram.

## Plot Google Visualization Chart of the first Quartile

```{r results='asis', tidy=FALSE}
q1classes<-subset(classTotals, freq <= 32 & freq >= 10)
orderedQ1Classes<-q1classes[with(q1classes, order(-freq)),]
hist<-gvisHistogram(orderedQ1Classes, options=list(width=1000, 
                                                  height=600, 
                                                  title='Class Size Density',
                                                  orientation='vertical'))
plot(hist)
```
The item number when you move the mouse pointer over each bar is the number of individual classes that have the number of students on the y-axis.  In this subset we have reduced the number of classes from **`r length(classTotals$learningInstance)`** to **`r length(q1classes$learningInstance)`** that appear to be 'real' classes rather than QA data.

***

## Knowledge Graphs
```{r tidy=TRUE}
KGs<-count(dataAll$studentId, "dataAll$knowledgeGraph")
orderedKGs<-KGs[with(KGs, order(-freq)),]
orderedKGs
```

```{r results='asis', tidy=FALSE}
bar<-gvisBarChart(orderedKGs, options=list(width=800, 
                                            height=600,
                                            title='Knowledge Graph Density',
                                            orientation='vertical'))
plot(bar)
```
There are **`r length(orderedKGs[[1]])`** Knowledge Graphs referenced in the message data but we only have two.  This will limit our ability to merge enriched data from the KGs into the message data.

***

## Total number of unique students and unique content in these data
```{r tidy=TRUE}
totStudents<-length(unique(dataAll$studentId))      
totContent<-length(unique(dataAll$contentId))       
```

There are **`r totStudents`** unique students and **`r totContent`** unique content objects in the raw data.

```{r tidy=TRUE}
passed<-subset(dataAll, dataAll$isCorrect=='Yes')
numStudentPassed<-length(unique(passed$studentId))
```

There are **`r numStudentPassed`** data sessions with passes for the assessments.

Now let's see how many of these received Knewton recommendations.

```{r tidy=TRUE}
recsReceived<-length(unique(subset(passed, receivedRecommendation=="TRUE")$studentId))
```

**`r recsReceived`** student sessions received Knewton recommendations.  We will break this down into the different recommendation type later.

Now let's remove the empty NA data - **note: why are there empty entries here - is this a bug in Adaptive Service?**

```{r tidy=TRUE}
passedTimeNa<-na.omit(passed$timeOnItem)
passedAttemptsNa<-na.omit(passed$numberAttempts)
```

Now we will look at the student sessions with failed assessments.

```{r tidy=TRUE}
failed<-subset(dataAll, dataAll$isCorrect=='No')
numStudentFailed<-length(unique(failed$studentId))

```{r tidy=TRUE}
failedTimeNa<-na.omit(failed$timeOnItem)
failedAttemptsNa<-na.omit(failed$numberAttempts)
```

There are **`r numStudentFailed`** data sessions with failed assessments.  We can do some summary statistics to see more detail.

```{r tidy=TRUE}
summary(failed)
summary(failed$timeOnItem)
summary(failed$numberAttempts)
```

How many of these received Knewton recommendations?

```{r tidy=TRUE}
recsReceived<-length(unique(subset(failed, receivedRecommendation=="TRUE")$studentId)) 
```

**`r recsReceived`** student sessions with failed assessments received Knewton recommendations.

## simple proportion of students who passed against students who failed

```{r results='asis', tidy=FALSE}
studentPieSummary<-data.frame(c('# Passed', '# Failed'), c(numStudentPassed, numStudentFailed))
ss<-gvisPieChart(studentPieSummary, options=list(title='Comparison of Students who passed/failed', width=800, height=800))
plot(ss)
```
We see slightly more than half of the unique student identities passes assessments.  Note that this data has not yet been normalized so we will have a number of student ids who passed also appearing in the failed area and vice-versa.

## Comparison of time spent on-task between students who passed and students who failed assessments

```{r tidy=TRUE}
boxplot(passed$timeOnItem, failed$timeOnItem, col=c("lightgreen", "pink"), title='Comparison statistics of time on task between passed and failed', xlab="Pass/Fail Cohorts (outliers removed)", ylab="Time On Task (Seconds)", names=c('Passed', 'Failed'), outline=FALSE)
```

There is a narrower spread of time spent on-task from the cohort of students who passed assessments and the mean time spent on-task is also lower.

***

```{r echo=FALSE, message=FALSE, results="hide"}
# This is to compare the various groupings
# From:- http://www.cookbook-r.com/Manipulating_data/Comparing_data_frames/
dupsBetweenGroups <- function (df, idcol) {
    # df: the data frame
    # idcol: the column which identifies the group each row belongs to
    
    # Get the data columns to use for finding matches
    datacols <- setdiff(names(df), idcol)
    
    # Sort by idcol, then datacols. Save order so we can undo the sorting later.
    sortorder <- do.call(order, df)
    df <- df[sortorder,]
    
    # Find duplicates within each id group (first copy not marked)
    dupWithin <- duplicated(df)
    
    # With duplicates within each group filtered out, find duplicates between groups. 
    # Need to scan up and down with duplicated() because first copy is not marked.
    dupBetween = rep(NA, nrow(df))
    dupBetween[!dupWithin] <- duplicated(df[!dupWithin,datacols])
    dupBetween[!dupWithin] <- duplicated(df[!dupWithin,datacols], fromLast=TRUE) | dupBetween[!dupWithin]
    
    
    # =================== Replace NA's with previous non-NA value =====================
    # This is why we sorted earlier - it was necessary to do this part efficiently
    
    # Get indexes of non-NA's
    goodIdx <- !is.na(dupBetween)
    
    # These are the non-NA values from x only
    # Add a leading NA for later use when we index into this vector
    goodVals <- c(NA, dupBetween[goodIdx])
    
    # Fill the indices of the output vector with the indices pulled from
    # these offsets of goodVals. Add 1 to avoid indexing to zero.
    fillIdx <- cumsum(goodIdx)+1
    
    # The original vector, now with gaps filled
    dupBetween <- goodVals[fillIdx]
    
    # Undo the original sort
    dupBetween[sortorder] <- dupBetween
    
    # Return the vector of which entries are duplicated across groups
    return(dupBetween)
}
```

## Recommendation adaptive types

```{r tidy=TRUE}
recs<-subset(dataAll, receivedRecommendation == 'TRUE', select=c(studentId, contentId, adaptiveType))
idx<-sapply(recs, is.character)
recs[idx] <- lapply(recs[idx], factor)
```

The adaptive type data is coded as a numerical factor with three levels (1, 2 and 3).  We need to convert adaptiveType to a string factor of three more meaningful levels (*warmup*, *intervention* and *enrichment*).

```{r results='asis', tidy=FALSE}
recs$adaptiveType<-factor(recs$adaptiveType)
levels(recs$adaptiveType)<-c("warmup", "intervention", "enrichment")

x<-summary(recs$adaptiveType)
y<-data.frame(names(x), x)
pieC<-gvisPieChart(y, options=list(width=800, height=800, title="Distribution of Adaptive Recommendation types in the data"))
plot(pieC)
```
There seem to be a large proportion of **intervention** recommendations compared with **enrichment** recommendations.  This would appear to be as expected given the large proportion of students who failed assessments.


```{r echo=FALSE, results="hide"}
# convert studentId levels from UUID to an integer number for easier human readibility
levels(recs$studentId)<-c(1:8083)
```

***

## Merge the content module information into the message data

We can enrich the message data with the categorical information contained in the Knowledge Graph data.  let's merge in the content names and descriptions that match with the content identity carried within the message session data.

```{r tidy=TRUE}
mdata<-merge(x=modules, y=dataAll, by.x="partner_module_id", by.y="contentId")

summary(mdata)
```


```{r echo=FALSE, results="hide"}
failed<-subset(mdata, mdata$isCorrect=='No' & !is.na(mdata$timeOnItem))
#summary(failed)                                                                 
```

```{r echo=FALSE, results="hide"}
# remove temporary variables
failed<-subset(failed, select=c(partner_module_id, module_name, Standard1, Standard2, DOK, studentId, learningInstance, isComplete, numberAttempts, timeOnItem, interactionEndTime, adaptiveType))
idx<-sapply(failed, is.character)
failed[idx] <- lapply(failed[idx], factor)
#summary(failed)
```


```{r echo=FALSE, results="hide"}
# Compare with students who passed
passed<-subset(mdata, mdata$isCorrect=='Yes' & !is.na(mdata$timeOnItem))
passed<-subset(passed, select=c(partner_module_id, module_name, Standard1, Standard2, DOK, studentId, learningInstance, isComplete, numberAttempts, timeOnItem, interactionEndTime, adaptiveType))
idx<-sapply(passed, is.character)
passed[idx] <- lapply(passed[idx], factor)
#summary(passed)
```

## Compare DOK with actual outcomes

The DOK stands for '**Depth Of Knowledge**' required.  It is therefore a measure of learning difficulty for a particular content object.

This analysis aims to compare the DOK distribution and see if the actual student outcomes in terms of 
pass/fail matches the expected DOK distribution.  A pie-chart is a good visual way to display this.

```{r tidy=TRUE}
failDOK<-subset(failed, select=c(DOK))
passDOK<-subset(passed, select=c(DOK))
totalDOK<-subset(mdata, !is.na(mdata$timeOnItem), select=c(DOK))
```

Comparative pie plots

```{r results='asis', tidy=FALSE}
x1<-summary(totalDOK$DOK)
y1<-data.frame(names(x1), x1)
y1<-subset(y1, x1>0)
title<-paste('DOK distribution for all Content in the data (', sum(y1$x1), ' items)', sep=' ')
dokT<-gvisPieChart(y1, options=list(title=title, width=500, height=500))

x2<-summary(passDOK$DOK)
y2<-data.frame(names(x2), x2)
y2<-subset(y2, x2>0)
title<-paste('DOK disribution for Content Passed (', sum(y2$x2), ' items)', sep=' ')
dokP<-gvisPieChart(y2, options=list(title=title, width=500, height=500))

x3<-summary(failDOK$DOK)
y3<-data.frame(names(x3), x3)
y3<-subset(y3, x3>0)
title<-paste('DOK distribution for Content Failed (', sum(y3$x3), ' items)', sep=' ')
dokF<-gvisPieChart(y3, options=list(title=title, width=500, height=500))

plot(gvisMerge(dokT, dokP, horizontal=TRUE))
```

```{r results='asis', tidy=FALSE, echo=FALSE}
title<-paste('DOK distribution for all Content in the data (', sum(y1$x1), ' items)', sep=' ')
dokT2<-gvisPieChart(y1, options=list(title=title, width=500, height=500))
plot(gvisMerge(dokT2, dokF, horizontal=TRUE))
```

The distribution of content passed seems very close to the total DOK distribution so it appears the DOK levels are appropriate based on student outcome for those students passing the assessments. But in the distribution of content failed the student outcomes suggest that the DOK level 2 content might be more difficult than the DOK level would assume.


Comparative bar plot

This is the same information above but in a single bar plot.

```{r tidy=TRUE}
yAll<-as.data.frame(cbind(y1$x1, y2$x2, y3$x3))
yAll<-cbind(yAll, y1$names.x1)
colnames(yAll) = c('V2', 'V3', 'V4', 'V1')
yAll<-subset(yAll, select=c(V1, V2:V4))
colnames(yAll) = c('DOK', 'Total Items', 'Passed Items', 'Failed Items')

library(ggplot2)
library(reshape2)
ylong<-melt(yAll)
ggplot(ylong,aes(DOK,value, fill=variable)) +
geom_bar(stat="identity",position="dodge")
```

The Pie Chart comparison appears to be easier to interpret in this case.

***

## Aggregate class pass/fail statistics between classes

```{r tidy=TRUE}
x<-subset(mdata, select=c(learningInstance, isCorrect))
idx<-sapply(x, is.character)
x[idx] <- lapply(x[idx], factor)
levels(x$learningInstance)<-c(1:length(x$learningInstance))
levels(x$isCorrect)<-c(1,2)

summary(x)

idx<-sapply(x, is.factor)
x[idx]<-lapply(x[idx], as.integer)

classes<-unique(x$learningInstance)
class<-integer(length(x[[1]]))
passed<-integer(length(x[[1]]))
failed<-integer(length(x[[1]]))
count<-1
for (i in 1:length(classes)) {
    tmp<-subset(x, learningInstance == classes[i])
    for (j in 1:length(tmp[[1]])) {
        class[count]<-tmp[j,]$learningInstance
        if (tmp[j,]$isCorrect == 1) {
            failed[count]<-1
            passed[count]<-NA
        } else {
            passed[count]<-1
            failed[count]<-NA
        } 
        count = count + 1
    } 
}
classesComp<-data.frame(class, passed, failed)

aggregatedPass<-aggregate(passed ~ class, FUN = sum, data=classesComp)
aggregatedFail<-aggregate(failed ~ class, FUN = sum, data=classesComp)

aggregatedPass$outcome<-"passed"
aggregatedFail$outcome<-"failed"

dataAllAggregated<-rbind.fill(aggregatedPass, aggregatedFail)

dupRows <- dupsBetweenGroups(dataAllAggregated,"outcome")

# Now separate out unique students in passed and failed categories
dataAllAggOutcome<-cbind(dataAllAggregated, unique=!dupRows)
dataAllAggOutcomePass<-subset(dataAllAggOutcome, outcome == 'passed' & unique == TRUE)
dataAllAggOutcomeFail<-subset(dataAllAggOutcome, outcome == 'failed' & unique == TRUE)

# Get rid of temporary data
dataAllAggOutcomeFail$passed<-NULL
dataAllAggOutcomeFail$outcome<-NULL
dataAllAggOutcomeFail$unique<-NULL
dataAllAggOutcomePass$failed<-NULL
dataAllAggOutcomePass$outcome<-NULL
dataAllAggOutcomePass$unique<-NULL

# order the results
PassOrdered<-dataAllAggOutcomePass[with(dataAllAggOutcomePass, order(-passed)),]
FailOrdered<-dataAllAggOutcomeFail[with(dataAllAggOutcomeFail, order(-failed)),]

boxplot(PassOrdered$passed, FailOrdered$failed, col=c("lightgreen", "pink"), title='Comparison statistics of unique students between passed and failed (outliers removed)', xlab="Pass/Fail Outcome", ylab="# Students in Class", names=c('Passed', 'Failed'), outline=FALSE)
```

This data indicates failing classes have fewer students than passing classes.  However, this data has not had the apparently QA data (very large class sizes) removed so no real conclusions can be drawn at this stage.  The large class filtered analysis will be done at a later time.

***

```{r resetOptions, echo=FALSE}
# reset Google Vis options
options(op)
```


